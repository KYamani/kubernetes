type: install
version: 1.5
id: kubernetes
baseUrl: https://raw.githubusercontent.com/jelastic-jps/kubernetes/master
description:
  text: /text/description-kube.md
  short: Kubernetes Cluster
categories:
  - apps/clusters
  - apps/dev-and-admin-tools

logo: /images/k8s-logo.png
name: Kubernetes Cluster
targetRegions:
  type: vz7

settings:
  fields:
    - name: deploy
      type: radio-fieldset
      values:
        cc: Clean cluster with pre-deployed HelloWorld example
        cmd: Deploy custom helm or stack via shell command
      default: cc
      showIf:
        cmd:
          name: cmd
          type: text
          height: 65
          required: true
          hideLabel: true
          default: |-
            helm repo add ibm-charts https://raw.githubusercontent.com/IBM/charts/master/repo/stable/
            helm install --name default --set autoscaling.enabled=true --set autoscaling.minReplicas=2 ibm-charts/ibm-open-liberty --version 1.6.0  --debug
            kubectl apply -f https://raw.githubusercontent.com/jelastic-jps/kubernetes/master/addons/openliberty.yaml

    - name: label
      caption: Topology
      type: displayfield
    - name: topo
      type: radio-fieldset
      values:
        0-dev: '<b>Development:</b> one master (1) and one scalable worker (1+)'
        1-prod: '<b>Production:</b> multi master (3) with API balancers (2+) and scalable workers (2+)'
      default: 0-dev

    - type: checkbox
      name: storage
      caption: Attach dedicated NFS Storage with dynamic volume provisioning
      value: true

    - type: checkbox
      name: api
      caption: Enable Remote API Access
      value: false

ssl: true

onBeforeInstall: |
  var url = "https://registry.hub.docker.com/v1/repositories/jelastic/kubernetes/tags",
    tags = toNative(new com.hivext.api.core.utils.Transport().get(url)).sort(),
    latest = "latest";
  for (var i = 0; i < tags.length; i++) if (tags[i].name > latest) latest = tags[i].name;
  var k8smCount = '${settings.topo}' == '0-dev' ? 1 : 3,
      workerCount = k8smCount > 1 ? 2 : 1;
  var resp = {
    result: 0,
    ssl: !!jelastic.billing.account.GetQuotas('environment.jelasticssl.enabled').array[0].value,
    nodes: [{
      count: k8smCount,
      cloudlets: 32,
      nodeType: "kubernetes",
      tag: "v1.14.3",
      scalingMode: "stateless",
      nodeGroup: "k8sm",
      displayName: "Master",
      extip: false,
      env: {
        JELASTIC_EXPOSE: false
      },
      volumes: [
        "/var/lib/connect"
      ],
      volumeMounts: {
        "/var/lib/connect": {
          readOnly: true,
          sourcePath: "/var/lib/connect",
          sourceNodeGroup: "k8sm"
        }
      }
    }, {
      count: workerCount,
      nodeGroup: "cp",
      nodeType: "kubernetes",
      tag: "v1.14.3",
      scalingMode: "stateless",
      displayName: "Workers",
      cloudlets: 32,
      extip: ${settings.extip:false},
      env: {
        JELASTIC_EXPOSE: false
      },
      volumes: [
        "/var/lib/connect"
      ],
      volumeMounts: {
        "/var/lib/connect": {
          readOnly: true,
          sourcePath: "/var/lib/connect",
          sourceNodeGroup: "k8sm"
        }
      }
    }]
  }

  if (k8smCount > 1) {
    resp.nodes.push({
      count: 2,
      nodeType: "haproxy",
      cloudlets: 8,
      displayName: "API Balancer",
      nodeGroup: "mbl",
      env: {
        JELASTIC_PORTS: 6443
      }
    })
  }

  if (${settings.storage:false}) {
    var path = "/data";
    resp.nodes.push({
      count: 1,
      nodeType: "storage",
      cloudlets: 8,
      displayName: "Storage",
      nodeGroup: "storage",
      volumes: [
        path
      ]
    })

    for (var i = 0; i < 2; i++){
      var n = resp.nodes[i];
      n.volumes.push(path);
      n.volumeMounts[path] = {
          readOnly: false,
          sourcePath: path,
          sourceNodeGroup: "storage"
      };
    }
  }
  return resp;

nodes: definedInOnBeforeInstall

skipNodeEmails: true

onInstall:
  - block-masters-scaling
  - init-main-master
  - forEach(node:nodes.k8sm):
      if (!${@node.ismaster}):
        init-slave-master:
          id: ${@node.id}
          ip: ${@node.intIP}

  - connect-workers: cp
  - setup-overlay
  - install-components
  - install-helm
  - install-traefik
  - manage-ingress-routes
  - generate-admin-token
  - connect-storage
  - deploy
  - install-conf-addon
  - remove-attr

onAfterScaleOut[cp]:
  forEach(event.response.nodes):
    connect-workers: ${@i.id}

onBeforeScaleIn[cp]:
  forEach(event.response.nodes):
    removeWorker:
      workerHostname: node${@i.id}-${env.domain}

onBeforeClone: stopEvent

actions:
  block-masters-scaling:
    env.control.ApplyNodeGroupData[k8sm]:
      data:
        validation:
          minCount: ${nodes.k8sm.length}
          maxCount: ${nodes.k8sm.length}

  init-main-master:
    - if (${nodes.mbl.length:0}):
        cmd[mbl]: |-
          sed -i '/^<\/mappings>.*/i \\t<pair frontend_port="6443" backend_port="6443" description="CPlane balancing" option="tcp-check" params="check fall 3 rise 2">' /etc/haproxy/tcpmaps/mappings.xml
          sed -i 's/^bind :::80/#bind :::80/g' /etc/haproxy/haproxy.cfg
          echo '${nodes.k8sm.master.intIP}' > /etc/haproxy/hosts
          jem balancer rebuildCommon
        user: root
    - cmd[${nodes.k8sm.master.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        entryPoint=$((( ${nodes.mbl.length:0} > 0 )) && echo mbl || echo k8sm)
        sed -i "s/^controlPlaneEndpoint:.*/controlPlaneEndpoint: \"${entryPoint}.${env.domain}:6443\"/g" /etc/kubernetes/custom-kubeadm.yaml
        kubeadm init --config /etc/kubernetes/custom-kubeadm.yaml --experimental-upload-certs --ignore-preflight-errors=swap | tee /var/log/kubeadm-init.log
        sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep 'experimental-control-plane' > /var/lib/connect/settings-master
        sed -n '/kubeadm join/,/^$/{/./p}' /var/log/kubeadm-init.log | sed ':a;N;$!ba;s/\\\n//g' | grep -v 'experimental-control-plane' > /var/lib/connect/settings
        printf "0 1 * * * kubeadm token create --print-join-command  > /var/lib/connect/settings\n\n\n" > /var/spool/cron/root
    - configure-master: ${nodes.k8sm.master.id}
    - if (${settings.api:true}):
        cmd[${nodes.k8sm.master.id}]: |-
           kubectl apply -f ${baseUrl}/addons/api-ingress.yaml

  init-slave-master:
    - cmd[${this.id}]: |-
        systemctl daemon-reload > /dev/null 2>&1
        $(cat /var/lib/connect/settings-master) --ignore-preflight-errors=swap > /dev/null 2>&1
    - configure-master: ${this.id}
    - cmd[mbl]: |-
        echo '${this.ip}' >> /etc/haproxy/hosts
        jem balancer rebuildCommon
      user: root

  configure-master:
    cmd[${this}]: |-
      mkdir -p $HOME/.kube
      cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
      chown root:root $HOME/.kube/config
      iptables -I INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
      service iptables save
      systemctl enable kubelet.service

  manage-ingress-routes:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/dashboard-ingress.yaml

  connect-workers:
    cmd[${this}]: |-
      systemctl daemon-reload > /dev/null 2>&1
      $(cat /var/lib/connect/settings) --ignore-preflight-errors=swap > /dev/null 2>&1
      sleep 5
      iptables -I INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
      service iptables save
      systemctl enable kubelet.service

  setup-overlay:
    cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/weave-pack.yaml
      wget https://github.com/weaveworks/weave/releases/download/v2.5.2/weave -O /usr/bin/weave
      chmod +x /usr/bin/weave

  install-components:
    - cmd[${nodes.k8sm.master.id}]: |-
        kubectl create -f ${baseUrl}/addons/metrics-server/aggregated-metrics-reader.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/auth-delegator.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/auth-reader.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/metrics-apiservice.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/metrics-server-deployment.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/metrics-server-service.yaml
        kubectl create -f ${baseUrl}/addons/metrics-server/resource-reader.yaml
        kubectl create -f ${baseUrl}/addons/kubernetes-dashboard.yaml
        kubectl create -f ${baseUrl}/addons/create-admin.yaml
        kubectl create -f ${baseUrl}/addons/grant-privileges.yaml

  install-helm:
    cmd[${nodes.k8sm.master.id}]: |-
      curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash > /dev/null 2>&1
      helm init --upgrade
      helm repo update
      kubectl create serviceaccount --namespace kube-system tiller
      kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
      kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
      while true; do kubectl get pods --field-selector=status.phase=Running -n kube-system | grep tiller && break ; done
      sleep 5

  install-traefik:
     cmd[${nodes.k8sm.master.id}]: |-
      kubectl apply -f ${baseUrl}/addons/traefik-rbac.yaml
      kubectl apply -f ${baseUrl}/addons/traefik-ds.yaml
      kubectl apply -f ${baseUrl}/addons/traefik-ui.yaml

  generate-admin-token:
    - cmd[${nodes.k8sm.master.id}]: kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep fulladmin | awk '{print $1}')  | grep 'token:' | sed -e's/token:\| //g'
    - setGlobals:
        token: ${response.out}

  deploy:
    - if ('${settings.deploy}' == 'cc'):
        cmd[${nodes.k8sm.master.id}]: |-
          kubectl apply -f ${baseUrl}/addons/helloworld.yaml
    - if ('${settings.deploy}' == 'cmd'):
        cmd[${nodes.k8sm.master.id}]: ${settings.cmd}
    - if ('${settings.deploy}' == 'yml'):
        cmd[${nodes.k8sm.master.id}]: kubectl apply -f ${settings.yml}

  connect-storage:
    if (${settings.storage:false}):
      cmd[${nodes.k8sm.master.id}]: helm install stable/nfs-client-provisioner --set nfs.server=${nodes.storage.master.address} --set nfs.path=/data --set replicaCount=3 --set storageClass.defaultClass=true --set storageClass.allowVolumeExpansion=true --set storageClass.name=jelastic-dynamic-volume


  removeWorker:
    cmd[${nodes.k8sm.master.id}]: |-
      /usr/bin/kubectl drain ${this.workerHostname} --ignore-daemonsets --delete-local-data || exit 8;
      /usr/bin/kubectl delete node ${this.workerHostname} || exit 9;
      
  remove-attr:
    cmd[*]: |-
      chattr -i -a /root/.bash_*
    user: root

  install-conf-addon:
    install:
      envName: ${env.envName}
      nodeGroup: k8sm
      jps:
        id: update-k8s-addon
        type: update
        baseUrl: ${baseUrl}
        name: Kubernetes Cluster Configuration
        description: Press "Remote API" to enable or disable remote access or "Update" to initiate update procedure.
        logo: /images/k8s-logo.png
        settings:
          fields:
            - type: displayfield
              caption: Please update this text
              name: dfremoteapi
              hideLabel: true
              markup: Add a clear description
            - type: checkbox
              name: remote
              caption: Enable Remote API Access
              value: ${settings.api}
        buttons:
          - caption: Remote API
            settings: main
            action: remoteApi
            loadingText: Updating...
            confirmText: Are you sure?
            successText: Remote API Access was successfully updated!
          - caption: Update
            action: update
            loadingText: Updating...
            confirmText: Do you want to update Kubernetes Cluster?
            successText: Kubernetes Cluster has been successfully updated!
        actions:
          update:
            script: |
              var envName = "${env.envName}", nodeId = "${nodes.k8sm.master.id}";
              var resp = jelastic.env.control.GetNodeInfo(envName, session, nodeId);
              if (resp.result != 0) return resp;
              var version = resp.node.version;
              var image = resp.node.name;
              resp = jelastic.env.control.GetContainerNodeTags(envName, session, nodeId);
              if (resp.result != 0) return resp;
              var tags = resp.object;
              var updates = [];
              for (var i = 0; i < tags.length; i++) if (tags[i] > version) updates.push(tags[i]);
              var message = "Current version " + version + " is the latest. No updates are available.";
              if (updates.length) {
                updates.sort();
                return {result:0, onAfterReturn:{execUpdate:{current:version, next:updates.shift(), avail:updates.join(", ")}}};
              } else {
                return {result:"info", message:message};
              }

          update-masters:
            - env.control.RedeployContainers:
                nodeId: ${this.id}
                tag: ${this.version}
                skipReinstall: true
            - cmd[${this.id}]: |-
                while true; do kubectl get nodes --no-headers | grep -qv '\sReady\s' || break ; done
                sleep 5
            - if (${this.master}):
                - cmd[${this.id}]: |-
                    /usr/bin/kubeadm upgrade plan --ignore-preflight-errors=swap || exit 1
                    /usr/bin/kubeadm upgrade apply ${this.version} || exit 1
                    service kubelet restart
            - if (!${this.master}):
                - cmd[${this.id}]: |-
                    /usr/bin/kubeadm upgrade node experimental-control-plane ${this.version}  || exit 2
                    service kubelet restart
                    
          update-workers:
            - cmd[${nodes.k8sm.master.id}]: |-
                while true; do kubectl get nodes --no-headers | grep -qv '\sReady\s' || break ; done
                sleep 5
                /usr/bin/kubectl drain ${this.hostname} --ignore-daemonsets --delete-local-data || exit 3
            - env.control.RedeployContainers:
                nodeId: ${this.id}
                tag: ${this.version}
                skipReinstall: true
            - cmd[${this.id}]: |-
                /usr/bin/kubeadm upgrade node config --kubelet-version ${this.version}  || exit 4
                service kubelet restart
            - cmd[${nodes.k8sm.master.id}]: |-
                /usr/bin/kubectl uncordon ${this.hostname} || exit 5
                while true; do kubectl get pods --field-selector=status.phase=Pending -n kube-system | grep -q Pending || break ; done

          execUpdate:
            - update-masters:
                id: ${nodes.k8sm.master.id}
                master: true
                version: ${this.next}
            - forEach(nodes.k8sm):
                if (!${@i.ismaster}):
                  update-masters:
                    id: ${@i.id}
                    master: false
                    version: ${this.next}
            - forEach(nodes.cp):
                update-workers:
                  id: ${@i.id}
                  hostname: node${@i.id}-${env.domain}
                  version: ${this.next}
            - script: |
                var message = "Kubernetes Cluster has been successfuly updated! **Current version:** ${this.next}.  \n\nNo other updates are available.";
                if ("${this.avail}") message += "\n\n**Next version:** ${this.avail}.  \nPress \"Update\" button to start the update process.";
                return {result:"info", message:message};

          remoteApi:
            cmd[${nodes.k8sm.master.id}]: |-
              action=$([ "${settings.remote}" == "true" ] && echo "apply" || echo "delete")
              kubectl $action -f ${baseUrl}/addons/api-ingress.yaml


success: /text/success.md
